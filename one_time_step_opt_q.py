# -*- coding: utf-8 -*-
"""one_time_step_opt_q.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jYlufiLYlxfR4Z31zBqc3Uprqgaxf2N9
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from random import uniform
import torch
from torch.optim import lr_scheduler
from tqdm import tqdm
from tqdm.notebook import tqdm
import time
import json
import copy
import glob

# @title
class ATA_tree_tensor:

    def __init__(self,dd, weights = [1], nodes = [0]):

        self.dim = dd
        self.n   = 2**dd
        self.nodes    = torch.tensor(nodes,dtype=torch.int32)
        self.weights  = torch.tensor(weights,dtype=torch.complex64)


    def get_bit_string(self,node_index):

        return torch.tensor([int(x) for x in format(node_index, 'b').zfill(self.dim)],dtype=torch.int32)


    def get_node_index(self,bit_str):

        s=''
        for x in bit_str: s += str(int(x))

        return int(s,2)


    def times_node(self,a_node,b_node):

        return self.get_node_index(torch.remainder(self.get_bit_string(a_node) + self.get_bit_string(b_node),2))


    def add_node(self,node_index, w = [0]):

        new_node = self.times_node(self.nodes[-1],node_index)

        if new_node not in self.nodes:

            self.nodes   = torch.cat((self.nodes,torch.tensor([new_node],dtype=torch.int32)))
            self.weights = torch.cat((self.weights,torch.tensor(w,dtype=torch.complex64)))


    def get_Z1(self,x):

        return (1-x)*torch.tensor([1,1],dtype=torch.int32) + x*torch.tensor([1,-1],dtype=torch.int32)


    def get_Z_all(self,bitstr):

        x = self.get_Z1(bitstr[0])

        for y in bitstr[1:]: x = torch.kron(x,self.get_Z1(y))

        return x


    def get_vector(self, w):

        z=torch.zeros(2**self.dim,dtype=torch.complex64)

        for x,y in zip(self.nodes, w):

            z += y*self.get_Z_all(self.get_bit_string(x))

        return z


    def get_state(self, node):

        return self.get_Z_all(self.get_bit_string(node))


    def cut(self, qvalue=0.1):

        cut          = torch.quantile(torch.abs(self.weights),qvalue)
        mask         = torch.abs(self.weights) > cut
        alpha        = torch.sum(torch.abs(self.weights[mask]))/torch.sum(torch.abs(self.weights))

        self.nodes   = self.nodes[mask]
        self.weights = self.weights[mask]

# @title
class ATA:

    def __init__(self, dd, cc, base_nodes):

        self.dim        = dd
        self.n          = 2**dd
        self.c          = cc
        self.tree       = ATA_tree_tensor(dd)
        self.base_nodes = base_nodes.copy()
        self.b          = torch.zeros(2**dd,dtype=torch.complex64)
        self.qft        = self.get_QFT(dd)
        print('QFT done!')
        self.true_sol   = torch.zeros(2**dd,dtype=torch.complex64)
        self.A_F        = torch.roll(torch.tensor([-self.c-4*np.pi**2*(ii-2**(dd-1))**2 /2**(2*dd) for ii in range(2**dd)],
                                            dtype=torch.complex64), 2**(dd-1))

        self.A_FF       = self.A_F.conj()*self.A_F


    def clearTree(self): self.tree = ATA_tree_tensor(self.dim)


    def get_QFT(self, n):

        w=np.exp(2j*np.pi/(2**n))
        gate=torch.zeros((2**n,2**n),dtype=torch.complex64)
        for i in range(2**n):
            for j in range(2**n):
                gate[i][j]=w**(i*j)

        return gate/np.sqrt(2**n)


    def get_true_sol(self):

        true_sol_F = (1/self.A_F)*self.b
        true_sol_F = true_sol_F/np.sqrt(torch.dot(true_sol_F.conj(),true_sol_F))

        return true_sol_F


    def get_L(self, n):

        matrix=torch.zeros((2**n,2**n),dtype=torch.complex64)
        for i in range(2**n):
            matrix[i][i-1]=1

        return matrix


    def get_Loss(self): return torch.real(self.Loss_function(self.tree.weights))

    def add_Node(self, node_index, w = [0]): self.tree.add_node(node_index, w = w)

    def set_b(self,b):

        self.b = b
        self.true_sol = self.get_true_sol()


    def get_inner_products(self):

        states = np.asarray([np.asarray(self.tree.get_state(node)*self.b,dtype=np.complex64) for node in self.tree.nodes])
        states = torch.tensor(states, dtype=torch.complex64)

        depth = len(states)

        inner1 = np.zeros(depth,dtype=np.complex64)
        inner2 = np.zeros((depth,depth),dtype=np.complex64)

        for nn in range(depth):

            inner1[nn] = states[nn].conj()@(self.A_F.conj()*self.b)


        for nn in range(depth):

            for mm in range(depth):

                if mm >= nn:

                    inner2[nn][mm] = states[nn].conj()@(self.A_FF*states[mm])

                else:

                    inner2[nn][mm] = (inner2[mm][nn]).conj()

        self.inner_1 = torch.tensor(inner1,dtype=torch.complex64)
        self.inner_2 = torch.tensor(inner2,dtype=torch.complex64)





    def Loss_function(self, w):
        x = self.tree.get_vector(w) * self.b
        # Добавляем случайный шум с относительной амплитудой 2%
        noise = 0.02 * torch.randn_like(x, dtype=torch.complex64)
        x_noisy = x + noise
        loss = (x_noisy.conj() @ (self.A_F.conj() * self.A_F * x_noisy)
                - self.b.conj() @ (self.A_F * x_noisy)
                - x_noisy.conj() @ (self.A_F.conj() * self.b)
                + 1)
        return torch.real(loss)


    def optimize(self, eps=1e-7, learning_rate = 0.1):

        wi   = self.tree.weights
        wi.requires_grad=True
        opt  = torch.optim.AdamW([wi], lr = learning_rate, weight_decay=1e-7)

        val0, val1 = 0, self.get_Loss().detach()
        scheduler=lr_scheduler.StepLR(opt, step_size=1000, gamma=0.95)


        counter = 0
        while np.abs(val1-val0) > eps:

            opt.zero_grad()
            loss=self.Loss_function(wi)
            loss.backward()
            opt.step()
            scheduler.step()
            counter += 1
#             print(counter,np.round(val1,4),end='\r')

            val0, val1 = val1, self.get_Loss().detach()

        self.tree.weights = wi.detach()


    def optimize2(self):

        self.get_inner_products()

        ww=torch.tensor(np.linalg.pinv(np.asarray(self.inner_2), hermitian = True, rcond = 0.0000001)@np.asarray(self.inner_1), dtype=torch.complex64)

        self.tree.weights = ww




    def get_Grad(self, node_index):

        child_node = self.tree.times_node(self.tree.nodes[-1],node_index)

        new_tree = ATA_tree_tensor(self.dim,
                                   nodes  = np.asarray([child_node],dtype=np.int32),
                                   weights= np.asarray([1], dtype = np.complex64))

        dx = new_tree.get_vector(new_tree.weights)*self.b
        x  = self.tree.get_vector(self.tree.weights)*self.b

        grad = (dx.conj()@(self.A_F.conj()*self.A_F*x)
               +x.conj()@(self.A_F.conj()*self.A_F*dx)
               -self.b.conj()@(self.A_F*dx)
               -dx.conj()@(self.A_F.conj()*self.b))

        return torch.abs(grad)


    def get_next_node(self):

        grad_overlaps = []

        for node in self.base_nodes: grad_overlaps.append(self.get_Grad(node))

        node_indexes=list(np.argsort(grad_overlaps))[::-1]

        nodes = [self.base_nodes[ii] for ii in node_indexes]

        for node in nodes:

            if self.tree.times_node(self.tree.nodes[-1],node) not in self.tree.nodes:

                return node

        print('SOS')





    def ATA_method(self, fidelity = 0.999, display_job = False, cut = False, len_tree = 0):

        fid = self.get_fidelity()

        while fid < fidelity:

            if cut and len(self.tree.nodes) > 10:

                self.tree.cut()

            self.optimize2()
            fid = self.get_fidelity()
            if (fid < fidelity) and (len_tree > 2**(self.dim)):
                return('не смог')
            next_node = self.get_next_node()
            self.add_Node(next_node)

            if display_job:

                loss    = np.real(self.get_Loss())
                nodes   = self.tree.nodes

                print('Fidelity: {0:.6f}, Loss: {1:.4f}, Nodes:{2}'.format(fid,  loss, len(nodes)), end='\r')

        return None


    def ATA_method_2(self, fidelity = 0.999, display_job = False, cut = False):

        fid = self.get_fidelity()

        while fid < fidelity:

            if cut and len(self.tree.nodes) > 10:

                self.tree.cut()


            self.optimize(learning_rate = 0.01)
            fid = self.get_fidelity()
            next_node = self.get_next_node()
            self.add_Node(next_node)

            if display_job:

                loss    = np.real(self.get_Loss())
                nodes   = self.tree.nodes

                print('Fidelity: {0:.6f}, Loss: {1:.4f}, Nodes:{2}'.format(fid,  loss, len(nodes)), end='\r')


        return None

    def get_fidelity(self):

        x_sol=self.tree.get_vector(self.tree.weights)*self.b
        x_sol=x_sol/np.sqrt(torch.dot(x_sol.conj(),x_sol))

        return (torch.abs(torch.dot(x_sol.conj(),self.true_sol))**2).item()


    def get_fidelity_history(self):

        fidelities = []

        for nn in range(1,len(self.tree.weights)+1):

            ww=self.tree.weights[0:nn]
            x_sol=self.tree.get_vector(ww)*self.b
            x_sol=x_sol/np.sqrt(torch.dot(x_sol.conj(),x_sol))

            fidelities.append((torch.abs(torch.dot(x_sol.conj(),self.true_sol))**2).item())

        return fidelities